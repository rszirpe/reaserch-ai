#!/usr/bin/env python3
"""
Background Trainer - Runs 24/7 training the model
Continuously scrapes web and improves the model
Run this script in the background: python background_trainer.py &
"""
import torch
import time
import sys
import os
from torch.utils.data import DataLoader

# Force unbuffered output
sys.stdout.reconfigure(line_buffering=True) if hasattr(sys.stdout, 'reconfigure') else None
sys.stderr.reconfigure(line_buffering=True) if hasattr(sys.stderr, 'reconfigure') else None

# Add current directory to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from model.neural_network import create_model
from model.tokenizer import SimpleTokenizer
from model.data_manager import DataManager
from model.autonomous_learner import AutonomousLearner
from model.trainer import ModelTrainer, QADataset
from model.grammar_corrector import GrammarCorrector
from model.quality_checker import QualityChecker
import config

def main():
    """Main 24/7 training loop"""
    print("\n" + "="*70)
    print("üß† 24/7 AUTONOMOUS AI TRAINER STARTING")
    print("="*70 + "\n")
    
    # Initialize components
    print("[1/7] Initializing Data Manager...")
    data_manager = DataManager(config.TRAINING_DB)
    
    print("[2/7] Initializing Tokenizer...")
    tokenizer = SimpleTokenizer(config.VOCAB_SIZE)
    
    # Try to load existing tokenizer
    if not tokenizer.load(config.VOCAB_PATH):
        print("   No existing tokenizer found. Will build from scratch.")
    
    print("[3/7] Creating Neural Network Model...")
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"   Using device: {device}")
    
    print(f"   Creating model with {config.VOCAB_SIZE} vocab...")
    model = create_model(
        vocab_size=config.VOCAB_SIZE,
        embedding_dim=config.EMBEDDING_DIM,
        hidden_dim=config.HIDDEN_DIM,
        num_layers=config.NUM_LAYERS
    )
    print(f"   Model created successfully!")
    
    print("[4/7] Initializing Trainer...")
    trainer = ModelTrainer(model, tokenizer, device)
    
    # Try to load existing checkpoint
    if os.path.exists(config.MODEL_PATH):
        trainer.load_checkpoint(config.MODEL_PATH)
    
    print("[5/7] Initializing Autonomous Learner...")
    learner = AutonomousLearner(data_manager)
    
    print("[6/7] Initializing Grammar Corrector...")
    grammar_corrector = GrammarCorrector()
    
    print("[7/7] Initializing Quality Checker...")
    quality_checker = QualityChecker(config.STATUS_FILE)
    
    print("\n‚úÖ All systems initialized!\n")
    print("="*70)
    print("üöÄ STARTING 24/7 TRAINING CYCLE")
    print("="*70)
    
    iteration = 0
    
    while True:
        try:
            iteration += 1
            print(f"\n{'='*70}")
            print(f"CYCLE {iteration} - {time.strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"{'='*70}")
            
            # Step 1: Autonomous learning (scrape web)
            print(f"\n[{iteration}.1] Autonomous web learning...")
            learner.learn_one_topic()
            
            total_examples = data_manager.get_total_examples()
            print(f"[{iteration}.1] Total examples collected: {total_examples}")
            
            # Step 2: Build/update vocabulary if needed
            if len(tokenizer) < 100 and total_examples >= 20:
                print(f"\n[{iteration}.2] Building vocabulary...")
                examples = data_manager.get_all_examples_for_vocab(limit=total_examples)
                all_texts = []
                for ex in examples:
                    all_texts.extend([ex[0], ex[1], ex[2]])  # question, context, answer
                print(f"[{iteration}.2] Building vocab from {len(all_texts)} texts...")
                tokenizer.build_vocab(all_texts)
                tokenizer.save(config.VOCAB_PATH)
            
            # Step 3: Train model if we have enough data
            untrained = data_manager.get_untrained_examples(limit=config.BATCH_SIZE)
            
            if len(untrained) >= config.BATCH_SIZE:
                print(f"\n[{iteration}.3] Training model on {len(untrained)} examples...")
                
                dataset = QADataset(untrained, tokenizer)
                dataloader = DataLoader(dataset, batch_size=min(8, len(untrained)), shuffle=True)
                
                loss = trainer.train_epoch(dataloader)
                print(f"[{iteration}.3] Training loss: {loss:.4f}")
                
                # Mark as trained
                example_ids = [ex[0] for ex in untrained]
                data_manager.mark_as_trained(example_ids)
                
                # Save checkpoint every 100 steps
                if trainer.train_step % config.CHECKPOINT_INTERVAL == 0:
                    trainer.save_checkpoint(config.MODEL_PATH)
            else:
                print(f"\n[{iteration}.3] Not enough data for training yet ({len(untrained)}/{config.BATCH_SIZE})")
            
            # Step 4: Evaluate model quality every 5 cycles
            if iteration % 5 == 0 and total_examples >= config.MIN_TRAINING_EXAMPLES:
                print(f"\n[{iteration}.4] Evaluating model quality...")
                
                # Simple quality estimate based on loss and examples
                avg_loss = trainer.get_avg_loss()
                quality_score = max(0, min(1, 1 - (avg_loss / 10)))  # Normalize loss to 0-1
                
                # Grammar score starts low, improves with more training
                grammar_score = min(0.95, quality_score + (total_examples / 10000))
                
                quality_checker.update_metrics(total_examples, quality_score, grammar_score)
                
                print(f"[{iteration}.4] Quality: {quality_score:.2%} | Grammar: {grammar_score:.2%}")
                print(f"[{iteration}.4] Status: {quality_checker.get_status_display()}")
                
                data_manager.save_performance(total_examples, quality_score, grammar_score, quality_checker.status['state'])
            
            # Display status
            print(f"\nüìä Status: {quality_checker.get_status_display()}")
            print(f"üíæ Model: {trainer.train_step} training steps")
            print(f"‚è±Ô∏è  Next cycle in {config.SCRAPE_INTERVAL} seconds...")
            
            # Sleep before next cycle
            time.sleep(config.SCRAPE_INTERVAL)
            
        except KeyboardInterrupt:
            print("\n\nüõë Training stopped by user")
            print("Saving final checkpoint...")
            trainer.save_checkpoint(config.MODEL_PATH)
            break
        except Exception as e:
            print(f"\n‚ùå Error in training cycle: {e}")
            import traceback
            traceback.print_exc()
            print(f"Retrying in {config.SCRAPE_INTERVAL} seconds...")
            time.sleep(config.SCRAPE_INTERVAL)


if __name__ == "__main__":
    print("\nüî• Starting 24/7 Background Training...")
    print("üí° This will run forever and continuously improve the model")
    print("üí° Press Ctrl+C to stop\n")
    
    main()

